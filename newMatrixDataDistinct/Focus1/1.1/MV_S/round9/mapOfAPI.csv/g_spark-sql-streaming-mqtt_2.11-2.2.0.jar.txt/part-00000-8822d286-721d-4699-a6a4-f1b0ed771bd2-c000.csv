0	org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1/1(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
1	scala/collection/Seq/isEmpty()
2	scala/collection/Seq$/empty()
3	org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/columnNames()
4	scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)
5	org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.reflect.ClassTag)
6	org/apache/spark/rdd/RDD/partitions()
7	org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$3/3(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
8	org/apache/spark/sql/execution/datasources/FileFormatWriter$/logInfo(scala.Function0)
9	scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)
10	scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
11	scala/collection/immutable/Range/size()
12	scala/collection/mutable/ArrayBuffer/isEmpty()
13	org/apache/spark/sql/execution/RangeExec/longMetric(java.lang.String)
14	org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()
15	scala/collection/mutable/ArrayBuffer/size()
16	java/io/DataInputStream/DataInputStream(java.io.InputStream)
17	org/apache/spark/sql/execution/SortExec/execute()
18	org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$resolvePartitions$2/2(scala.collection.immutable.IndexedSeq)
19	org/apache/spark/internal/io/FileCommitProtocol/abortJob(org.apache.hadoop.mapreduce.JobContext)
20	org/apache/spark/internal/io/FileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)
21	scala/collection/Seq/size()
22	scala/math/package$/min(long,long)
23	scala/runtime/RichInt$/until$extension0(int,int)
24	org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)
25	scala/reflect/ClassTag$/apply(java.lang.Class)
26	org/apache/spark/sql/execution/RangeExec/sqlContext()
27	org/apache/spark/sql/execution/RangeExec$$anonfun$20/20(org.apache.spark.sql.execution.RangeExec,org.apache.spark.sql.execution.metric.SQLMetric)
28	org/apache/spark/sql/internal/SQLConf/limitScaleUpFactor()
29	org/apache/spark/sql/execution/SparkPlan$$anonfun$executeTake$1/1(org.apache.spark.sql.execution.SparkPlan)
30	scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
31	scala/Predef$/intWrapper(int)
32	org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$2/2(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1,org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult[])
33	org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$15/15(scala.collection.Seq)
34	scala/Function1/apply(java.lang.Object)
35	org/apache/spark/internal/io/FileCommitProtocol/setupJob(org.apache.hadoop.mapreduce.JobContext)
36	scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
37	org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)
38	scala/Predef$/wrapRefArray(java.lang.Object[])
39	org/apache/spark/api/r/SerDe$/readInt(java.io.DataInputStream)
40	scala/collection/mutable/ArrayOps/distinct()
41	java/io/ByteArrayInputStream/ByteArrayInputStream(byte[])
42	org/apache/spark/sql/execution/datasources/FileFormatWriter$/logError(scala.Function0,java.lang.Throwable)
43	scala/collection/Seq$/canBuildFrom()
44	scala/Predef$DummyImplicit$/dummyImplicit()
45	org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$13/13(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
46	scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
47	org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$15/15(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
48	org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$14/14()
49	org/apache/spark/sql/Row$/fromSeq(scala.collection.Seq)
50	org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)
51	org/apache/spark/sql/execution/datasources/PartitioningUtils$/resolvePartitions(scala.collection.Seq)
52	org/apache/spark/sql/execution/SparkPlan$$anonfun$3/3(org.apache.spark.sql.execution.SparkPlan)
53	scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)
54	scala/collection/mutable/ArrayBuffer/take(int)
55	org/apache/spark/sql/api/r/SQLUtils$$anonfun$bytesToRow$1/1(org.apache.spark.sql.types.StructType,java.io.DataInputStream)
56	scala/reflect/ClassTag$/Int()
57	scala/collection/Seq/head()
58	scala/Array$/canBuildFrom(scala.reflect.ClassTag)
59	org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$resolvePartitions$1/1(scala.collection.Seq)
60	scala/Predef$/assert(boolean,scala.Function0)
61	org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.collection.Seq,scala.Function2,scala.reflect.ClassTag)
62	java/lang/Math/max(int,int)
63	java/lang/Math/min(long,long)
64	scala/Predef$/refArrayOps(java.lang.Object[])
65	scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
66	org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$4/4(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
67	org/apache/spark/sql/execution/SortExec$/apply$default$4()
68	scala/collection/SeqLike/distinct()
69	scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
70	scala/collection/mutable/ArrayBuffer/ArrayBuffer()
71	scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
72	scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
73	scala/collection/immutable/IndexedSeq$/canBuildFrom()
74	org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$13/13()
75	org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$12/12(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
76	org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$14/14(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
